apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: titanic-ml-u1rng
  annotations:
    tekton.dev/output_artifacts: '{"datapreprocessing": [{"key": "artifacts/$PIPELINERUN/datapreprocessing/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"},
      {"key": "artifacts/$PIPELINERUN/datapreprocessing/datapreprocessing.tgz", "name":
      "datapreprocessing", "path": "/tmp/datapreprocessing.html"}], "decisiontree":
      [{"key": "artifacts/$PIPELINERUN/decisiontree/mlpipeline-ui-metadata.tgz", "name":
      "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"}, {"key":
      "artifacts/$PIPELINERUN/decisiontree/decisiontree.tgz", "name": "decisiontree",
      "path": "/tmp/decisiontree.html"}], "featureengineering": [{"key": "artifacts/$PIPELINERUN/featureengineering/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"},
      {"key": "artifacts/$PIPELINERUN/featureengineering/featureengineering.tgz",
      "name": "featureengineering", "path": "/tmp/featureengineering.html"}], "loaddata":
      [{"key": "artifacts/$PIPELINERUN/loaddata/mlpipeline-ui-metadata.tgz", "name":
      "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"}, {"key":
      "artifacts/$PIPELINERUN/loaddata/loaddata.tgz", "name": "loaddata", "path":
      "/tmp/loaddata.html"}], "logisticregression": [{"key": "artifacts/$PIPELINERUN/logisticregression/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"},
      {"key": "artifacts/$PIPELINERUN/logisticregression/logisticregression.tgz",
      "name": "logisticregression", "path": "/tmp/logisticregression.html"}], "naivebayes":
      [{"key": "artifacts/$PIPELINERUN/naivebayes/mlpipeline-ui-metadata.tgz", "name":
      "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"}, {"key":
      "artifacts/$PIPELINERUN/naivebayes/naivebayes.tgz", "name": "naivebayes", "path":
      "/tmp/naivebayes.html"}], "randomforest": [{"key": "artifacts/$PIPELINERUN/randomforest/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"},
      {"key": "artifacts/$PIPELINERUN/randomforest/randomforest.tgz", "name": "randomforest",
      "path": "/tmp/randomforest.html"}], "results": [{"key": "artifacts/$PIPELINERUN/results/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"},
      {"key": "artifacts/$PIPELINERUN/results/results.tgz", "name": "results", "path":
      "/tmp/results.html"}], "svm": [{"key": "artifacts/$PIPELINERUN/svm/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"},
      {"key": "artifacts/$PIPELINERUN/svm/svm.tgz", "name": "svm", "path": "/tmp/svm.html"}]}'
    tekton.dev/input_artifacts: '{"datapreprocessing": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "decisiontree": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "featureengineering": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "loaddata": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "logisticregression": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "naivebayes": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "randomforest": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "results": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "svm": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"datapreprocessing": [["mlpipeline-ui-metadata",
      "/tmp/mlpipeline-ui-metadata.json"], ["datapreprocessing", "/tmp/datapreprocessing.html"]],
      "decisiontree": [["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"],
      ["decisiontree", "/tmp/decisiontree.html"]], "featureengineering": [["mlpipeline-ui-metadata",
      "/tmp/mlpipeline-ui-metadata.json"], ["featureengineering", "/tmp/featureengineering.html"]],
      "kale-marshal-volume": [], "loaddata": [["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"],
      ["loaddata", "/tmp/loaddata.html"]], "logisticregression": [["mlpipeline-ui-metadata",
      "/tmp/mlpipeline-ui-metadata.json"], ["logisticregression", "/tmp/logisticregression.html"]],
      "naivebayes": [["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"],
      ["naivebayes", "/tmp/naivebayes.html"]], "randomforest": [["mlpipeline-ui-metadata",
      "/tmp/mlpipeline-ui-metadata.json"], ["randomforest", "/tmp/randomforest.html"]],
      "results": [["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"],
      ["results", "/tmp/results.html"]], "svm": [["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"],
      ["svm", "/tmp/svm.html"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Predict which passengers
      survived the Titanic shipwreck", "name": "titanic-ml-u1rng"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  pipelineSpec:
    tasks:
    - name: kale-marshal-volume
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: $(PIPELINERUN)-kale-marshal-pvc
            spec:
              accessModes:
              - ReadWriteMany
              resources:
                requests:
                  storage: 1Gi
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations: {}
    - name: loaddata
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def loaddata():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                path = "data/"
                PREDICTION_LABEL = 'Survived'
                test_df = pd.read_csv(path + "test.csv")
                train_df = pd.read_csv(path + "train.csv")
                '''

                _kale_data_saving_block = '''
                # -----------------------DATA SAVING START---------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                _kale_marshal_utils.save(PREDICTION_LABEL, "PREDICTION_LABEL")
                _kale_marshal_utils.save(test_df, "test_df")
                _kale_marshal_utils.save(train_df, "train_df")
                # -----------------------DATA SAVING END-----------------------------------
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (
                    _kale_block1,
                    _kale_block2,
                    _kale_data_saving_block)
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/loaddata.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('loaddata')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Loaddata', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = loaddata(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Loaddata", "outputs":
              [], "version": "Loaddata@sha256=656346194c93dede0c7caa8b34302358f5796d089486d58d42b2c1f6aaa29ff8"}'
    - name: datapreprocessing
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def datapreprocessing():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_data_loading_block = '''
                # -----------------------DATA LOADING START--------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                test_df = _kale_marshal_utils.load("test_df")
                train_df = _kale_marshal_utils.load("train_df")
                # -----------------------DATA LOADING END----------------------------------
                '''

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                data = [train_df, test_df]
                for dataset in data:
                    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']
                    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0
                    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1
                    dataset['not_alone'] = dataset['not_alone'].astype(int)
                train_df['not_alone'].value_counts()
                '''

                _kale_block3 = '''
                # This does not contribute to a person survival probability
                train_df = train_df.drop(['PassengerId'], axis=1)
                '''

                _kale_block4 = '''
                import re
                deck = {"A": 1, "B": 2, "C": 3, "D": 4, "E": 5, "F": 6, "G": 7, "U": 8}
                data = [train_df, test_df]
                for dataset in data:
                    dataset['Cabin'] = dataset['Cabin'].fillna("U0")
                    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile("([a-zA-Z]+)").search(x).group())
                    dataset['Deck'] = dataset['Deck'].map(deck)
                    dataset['Deck'] = dataset['Deck'].fillna(0)
                    dataset['Deck'] = dataset['Deck'].astype(int)
                # we can now drop the cabin feature
                train_df = train_df.drop(['Cabin'], axis=1)
                test_df = test_df.drop(['Cabin'], axis=1)
                '''

                _kale_block5 = '''
                data = [train_df, test_df]
                for dataset in data:
                    mean = train_df["Age"].mean()
                    std = test_df["Age"].std()
                    is_null = dataset["Age"].isnull().sum()
                    # compute random numbers between the mean, std and is_null
                    rand_age = np.random.randint(mean - std, mean + std, size = is_null)
                    # fill NaN values in Age column with random values generated
                    age_slice = dataset["Age"].copy()
                    age_slice[np.isnan(age_slice)] = rand_age
                    dataset["Age"] = age_slice
                    dataset["Age"] = train_df["Age"].astype(int)
                train_df["Age"].isnull().sum()
                '''

                _kale_block6 = '''
                train_df['Embarked'].describe()
                '''

                _kale_block7 = '''
                # fill with most common value
                common_value = 'S'
                data = [train_df, test_df]
                for dataset in data:
                    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)
                '''

                _kale_block8 = '''
                train_df.info()
                '''

                _kale_data_saving_block = '''
                # -----------------------DATA SAVING START---------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                _kale_marshal_utils.save(test_df, "test_df")
                _kale_marshal_utils.save(train_df, "train_df")
                # -----------------------DATA SAVING END-----------------------------------
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (_kale_data_loading_block,
                                _kale_block1,
                                _kale_block2,
                                _kale_block3,
                                _kale_block4,
                                _kale_block5,
                                _kale_block6,
                                _kale_block7,
                                _kale_block8,
                                _kale_data_saving_block)
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/datapreprocessing.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('datapreprocessing')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Datapreprocessing', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = datapreprocessing(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["loaddata", "kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Datapreprocessing",
              "outputs": [], "version": "Datapreprocessing@sha256=17a25f5227a4f88e535f59bdeadceeace192ed89670054921466fc675fd5c292"}'
      runAfter:
      - loaddata
    - name: featureengineering
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def featureengineering():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_data_loading_block = '''
                # -----------------------DATA LOADING START--------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                PREDICTION_LABEL = _kale_marshal_utils.load("PREDICTION_LABEL")
                test_df = _kale_marshal_utils.load("test_df")
                train_df = _kale_marshal_utils.load("train_df")
                # -----------------------DATA LOADING END----------------------------------
                '''

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                data = [train_df, test_df]
                for dataset in data:
                    dataset['Fare'] = dataset['Fare'].fillna(0)
                    dataset['Fare'] = dataset['Fare'].astype(int)
                '''

                _kale_block3 = '''
                data = [train_df, test_df]
                titles = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}
                for dataset in data:
                    # extract titles
                    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)
                    # replace titles with a more common title or as Rare
                    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\
                                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
                    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
                    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
                    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
                    # convert titles into numbers
                    dataset['Title'] = dataset['Title'].map(titles)
                    # filling NaN with 0, to get safe
                    dataset['Title'] = dataset['Title'].fillna(0)
                train_df = train_df.drop(['Name'], axis=1)
                test_df = test_df.drop(['Name'], axis=1)
                '''

                _kale_block4 = '''
                genders = {"male": 0, "female": 1}
                data = [train_df, test_df]
                for dataset in data:
                    dataset['Sex'] = dataset['Sex'].map(genders)
                '''

                _kale_block5 = '''
                train_df = train_df.drop(['Ticket'], axis=1)
                test_df = test_df.drop(['Ticket'], axis=1)
                '''

                _kale_block6 = '''
                ports = {"S": 0, "C": 1, "Q": 2}
                data = [train_df, test_df]
                for dataset in data:
                    dataset['Embarked'] = dataset['Embarked'].map(ports)
                '''

                _kale_block7 = '''
                data = [train_df, test_df]
                for dataset in data:
                    dataset['Age'] = dataset['Age'].astype(int)
                    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0
                    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1
                    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2
                    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3
                    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4
                    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5
                    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6
                    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6
                # let's see how it's distributed train_df['Age'].value_counts()
                '''

                _kale_block8 = '''
                data = [train_df, test_df]
                for dataset in data:
                    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0
                    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1
                    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2
                    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3
                    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4
                    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5
                    dataset['Fare'] = dataset['Fare'].astype(int)
                '''

                _kale_block9 = '''
                data = [train_df, test_df]
                for dataset in data:
                    dataset['Age_Class']= dataset['Age']* dataset['Pclass']
                '''

                _kale_block10 = '''
                for dataset in data:
                    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)
                    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)
                # Let's take a last look at the training set, before we start training the models.
                train_df.head(10)
                '''

                _kale_block11 = '''
                train_labels = train_df[PREDICTION_LABEL]
                train_df = train_df.drop(PREDICTION_LABEL, axis=1)
                '''

                _kale_data_saving_block = '''
                # -----------------------DATA SAVING START---------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                _kale_marshal_utils.save(train_df, "train_df")
                _kale_marshal_utils.save(train_labels, "train_labels")
                # -----------------------DATA SAVING END-----------------------------------
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (_kale_data_loading_block,
                                _kale_block1,
                                _kale_block2,
                                _kale_block3,
                                _kale_block4,
                                _kale_block5,
                                _kale_block6,
                                _kale_block7,
                                _kale_block8,
                                _kale_block9,
                                _kale_block10,
                                _kale_block11,
                                _kale_data_saving_block)
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/featureengineering.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('featureengineering')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Featureengineering', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = featureengineering(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["datapreprocessing", "kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Featureengineering",
              "outputs": [], "version": "Featureengineering@sha256=6e5d2eccf0a1e5d2381a46b1e7ef009cef6bd917efadb9a8fca35bde68406d3e"}'
      runAfter:
      - datapreprocessing
    - name: decisiontree
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def decisiontree():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_data_loading_block = '''
                # -----------------------DATA LOADING START--------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                train_df = _kale_marshal_utils.load("train_df")
                train_labels = _kale_marshal_utils.load("train_labels")
                # -----------------------DATA LOADING END----------------------------------
                '''

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                decision_tree = DecisionTreeClassifier()
                decision_tree.fit(train_df, train_labels)
                acc_decision_tree = round(decision_tree.score(train_df, train_labels) * 100, 2)
                '''

                _kale_data_saving_block = '''
                # -----------------------DATA SAVING START---------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                _kale_marshal_utils.save(acc_decision_tree, "acc_decision_tree")
                # -----------------------DATA SAVING END-----------------------------------
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (_kale_data_loading_block,
                                _kale_block1,
                                _kale_block2,
                                _kale_data_saving_block)
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/tmp/decisiontree.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('decisiontree')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Decisiontree', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = decisiontree(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["featureengineering", "kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Decisiontree",
              "outputs": [], "version": "Decisiontree@sha256=83fb59c8529dcd42d9f57ec9a30443911d3839b746c4af8e1a00ba614b16ee55"}'
      runAfter:
      - featureengineering
    - name: svm
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def svm():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_data_loading_block = '''
                # -----------------------DATA LOADING START--------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                train_df = _kale_marshal_utils.load("train_df")
                train_labels = _kale_marshal_utils.load("train_labels")
                # -----------------------DATA LOADING END----------------------------------
                '''

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                linear_svc = SVC(gamma='auto')
                linear_svc.fit(train_df, train_labels)
                acc_linear_svc = round(linear_svc.score(train_df, train_labels) * 100, 2)
                '''

                _kale_data_saving_block = '''
                # -----------------------DATA SAVING START---------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                _kale_marshal_utils.save(acc_linear_svc, "acc_linear_svc")
                # -----------------------DATA SAVING END-----------------------------------
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (_kale_data_loading_block,
                                _kale_block1,
                                _kale_block2,
                                _kale_data_saving_block)
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/tmp/svm.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('svm')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Svm', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = svm(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["featureengineering", "kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Svm", "outputs":
              [], "version": "Svm@sha256=38e10f685ab4048040453f9346c7f901e26686e0a7a06783d2f8a863db0c9760"}'
      runAfter:
      - featureengineering
    - name: naivebayes
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def naivebayes():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_data_loading_block = '''
                # -----------------------DATA LOADING START--------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                train_df = _kale_marshal_utils.load("train_df")
                train_labels = _kale_marshal_utils.load("train_labels")
                # -----------------------DATA LOADING END----------------------------------
                '''

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                gaussian = GaussianNB()
                gaussian.fit(train_df, train_labels)
                acc_gaussian = round(gaussian.score(train_df, train_labels) * 100, 2)
                '''

                _kale_data_saving_block = '''
                # -----------------------DATA SAVING START---------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                _kale_marshal_utils.save(acc_gaussian, "acc_gaussian")
                # -----------------------DATA SAVING END-----------------------------------
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (_kale_data_loading_block,
                                _kale_block1,
                                _kale_block2,
                                _kale_data_saving_block)
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/tmp/naivebayes.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('naivebayes')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Naivebayes', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = naivebayes(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["featureengineering", "kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Naivebayes",
              "outputs": [], "version": "Naivebayes@sha256=5ad60275d2a6cfdf13682c1f33ef52c0dcb0492aa851f023c7fe7863be60538e"}'
      runAfter:
      - featureengineering
    - name: logisticregression
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def logisticregression():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_data_loading_block = '''
                # -----------------------DATA LOADING START--------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                train_df = _kale_marshal_utils.load("train_df")
                train_labels = _kale_marshal_utils.load("train_labels")
                # -----------------------DATA LOADING END----------------------------------
                '''

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                logreg = LogisticRegression(solver='lbfgs', max_iter=110)
                logreg.fit(train_df, train_labels)
                acc_log = round(logreg.score(train_df, train_labels) * 100, 2)
                '''

                _kale_data_saving_block = '''
                # -----------------------DATA SAVING START---------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                _kale_marshal_utils.save(acc_log, "acc_log")
                # -----------------------DATA SAVING END-----------------------------------
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (_kale_data_loading_block,
                                _kale_block1,
                                _kale_block2,
                                _kale_data_saving_block)
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/tmp/logisticregression.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('logisticregression')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Logisticregression', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = logisticregression(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["featureengineering", "kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Logisticregression",
              "outputs": [], "version": "Logisticregression@sha256=70a61efec5e84940af9dccea55cb672c33545c2390ef35d37778f787740b7d66"}'
      runAfter:
      - featureengineering
    - name: randomforest
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def randomforest():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_data_loading_block = '''
                # -----------------------DATA LOADING START--------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                train_df = _kale_marshal_utils.load("train_df")
                train_labels = _kale_marshal_utils.load("train_labels")
                # -----------------------DATA LOADING END----------------------------------
                '''

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                random_forest = RandomForestClassifier(n_estimators=100)
                random_forest.fit(train_df, train_labels)
                acc_random_forest = round(random_forest.score(train_df, train_labels) * 100, 2)
                '''

                _kale_data_saving_block = '''
                # -----------------------DATA SAVING START---------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                _kale_marshal_utils.save(acc_random_forest, "acc_random_forest")
                # -----------------------DATA SAVING END-----------------------------------
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (_kale_data_loading_block,
                                _kale_block1,
                                _kale_block2,
                                _kale_data_saving_block)
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/tmp/randomforest.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('randomforest')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Randomforest', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = randomforest(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["featureengineering", "kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Randomforest",
              "outputs": [], "version": "Randomforest@sha256=15322623fd759edd3f9ea22f85239b1c6e81f38c1740b436584744002c516d30"}'
      runAfter:
      - featureengineering
    - name: results
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def results():
                from kale.common import mlmdutils as _kale_mlmdutils
                _kale_mlmdutils.init_metadata()

                _kale_data_loading_block = '''
                # -----------------------DATA LOADING START--------------------------------
                from kale.marshal import utils as _kale_marshal_utils
                _kale_marshal_utils.set_kale_data_directory("/marshal")
                acc_decision_tree = _kale_marshal_utils.load("acc_decision_tree")
                acc_gaussian = _kale_marshal_utils.load("acc_gaussian")
                acc_linear_svc = _kale_marshal_utils.load("acc_linear_svc")
                acc_log = _kale_marshal_utils.load("acc_log")
                acc_random_forest = _kale_marshal_utils.load("acc_random_forest")
                # -----------------------DATA LOADING END----------------------------------
                '''

                _kale_block1 = '''
                import numpy as np
                import pandas as pd
                import seaborn as sns
                from matplotlib import pyplot as plt
                from matplotlib import style
                from sklearn import linear_model
                from sklearn.linear_model import LogisticRegression
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.linear_model import Perceptron
                from sklearn.linear_model import SGDClassifier
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.naive_bayes import GaussianNB
                '''

                _kale_block2 = '''
                results = pd.DataFrame({
                    'Model': ['Support Vector Machines', 'logistic Regression',
                              'Random Forest', 'Naive Bayes', 'Decision Tree'],
                    'Score': [acc_linear_svc, acc_log,
                              acc_random_forest, acc_gaussian, acc_decision_tree]})
                result_df = results.sort_values(by='Score', ascending=False)
                result_df = result_df.set_index('Score')
                print(result_df)
                '''

                # run the code blocks inside a jupyter kernel
                from kale.common.jputils import run_code as _kale_run_code
                from kale.common.kfputils import \
                    update_uimetadata as _kale_update_uimetadata
                _kale_blocks = (_kale_data_loading_block,
                                _kale_block1,
                                _kale_block2,
                                )
                _kale_html_artifact = _kale_run_code(_kale_blocks)
                with open("/tmp/results.html", "w") as f:
                    f.write(_kale_html_artifact)
                _kale_update_uimetadata('results')

                _kale_mlmdutils.call("mark_execution_complete")

            import argparse
            _parser = argparse.ArgumentParser(prog='Results', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = results(**_parsed_args)
          image: python:3.7
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/~/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/metadata_written: "true"
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            kubeflow-kale.org/dependent-templates: '["randomforest", "logisticregression",
              "naivebayes", "svm", "decisiontree", "kale-marshal-volume"]'
            kubeflow-kale.org/volume-name-parameters: '["kale-marshal-volume-name"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Results", "outputs":
              [], "version": "Results@sha256=3d50d7089f67ff3f576c8e75282756c98696d342704e72756e204e018fbf99ad"}'
      runAfter:
      - randomforest
      - logisticregression
      - naivebayes
      - svm
      - decisiontree
